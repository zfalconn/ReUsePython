{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba8499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "MODEL_PATH = r\"models\\focus1\\Focus1_YOLO11s_x1024_14112024.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a457f299",
   "metadata": {},
   "source": [
    "# Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb0b6007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.162  Python-3.11.9 torch-2.7.1+cu128 CPU (Intel Core(TM) Ultra 7 165H)\n",
      "YOLO11s summary (fused): 100 layers, 9,413,574 parameters, 0 gradients, 21.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'models\\focus1\\Focus1_YOLO11s_x1024_14112024.pt' with input shape (1, 3, 1024, 1024) BCHW and output shape(s) (1, 6, 21504) (18.4 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['ai-edge-litert>=1.2.0,<1.4.0'] not found, attempting AutoUpdate...\n",
      "WARNING Retry 1/2 failed: Command 'pip install --no-cache-dir \"ai-edge-litert>=1.2.0,<1.4.0\" --extra-index-url https://pypi.ngc.nvidia.com' returned non-zero exit status 1.\n",
      "WARNING Retry 2/2 failed: Command 'pip install --no-cache-dir \"ai-edge-litert>=1.2.0,<1.4.0\" --extra-index-url https://pypi.ngc.nvidia.com' returned non-zero exit status 1.\n",
      "WARNING \u001b[31m\u001b[1mrequirements:\u001b[0m  Command 'pip install --no-cache-dir \"ai-edge-litert>=1.2.0,<1.4.0\" --extra-index-url https://pypi.ngc.nvidia.com' returned non-zero exit status 1.\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.19.0...\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.59...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  5.0s, saved as 'models\\focus1\\Focus1_YOLO11s_x1024_14112024.onnx' (36.7 MB)\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m collecting INT8 calibration images from 'data=Focus1\\Focus1\\new dataset_2000\\batrec.v9-allen_yolo8_11112024.yolov8-obb\\data.yaml'\n",
      "Fast image access  (ping: 0.10.0 ms, read: 88.017.2 MB/s, size: 19.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning C:\\Users\\lin40269\\Desktop\\Linh\\01_Python\\realsense\\Focus1\\Focus1\\new dataset_2000\\batrec.v9-allen_yolo8_11112024.yolov8-obb\\valid\\labels.cache... 213 images, 0 backgrounds, 0 corrupt: 100%|██████████| 213/213 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING \u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m >300 images recommended for INT8 calibration, found 213 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lin40269\\Desktop\\Linh\\01_Python\\realsense\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "ERROR \u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export failure 46.7s: No module named 'ai_edge_litert'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ai_edge_litert'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model = YOLO(MODEL_PATH)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtflite\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFocus1\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mFocus1\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mnew dataset_2000\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mbatrec.v9-allen_yolo8_11112024.yolov8-obb\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mdata.yaml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint8\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lin40269\\Desktop\\Linh\\01_Python\\realsense\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:734\u001b[39m, in \u001b[36mModel.export\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    726\u001b[39m custom = {\n\u001b[32m    727\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimgsz\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.model.args[\u001b[33m\"\u001b[39m\u001b[33mimgsz\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    728\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    731\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mverbose\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    732\u001b[39m }  \u001b[38;5;66;03m# method defaults\u001b[39;00m\n\u001b[32m    733\u001b[39m args = {**\u001b[38;5;28mself\u001b[39m.overrides, **custom, **kwargs, \u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mexport\u001b[39m\u001b[33m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExporter\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lin40269\\Desktop\\Linh\\01_Python\\realsense\\.venv\\Lib\\site-packages\\ultralytics\\engine\\exporter.py:487\u001b[39m, in \u001b[36mExporter.__call__\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tf_format:  \u001b[38;5;66;03m# TensorFlow formats\u001b[39;00m\n\u001b[32m    486\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.int8 |= edgetpu\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     f[\u001b[32m5\u001b[39m], keras_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexport_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pb \u001b[38;5;129;01mor\u001b[39;00m tfjs:  \u001b[38;5;66;03m# pb prerequisite to tfjs\u001b[39;00m\n\u001b[32m    489\u001b[39m         f[\u001b[32m6\u001b[39m], _ = \u001b[38;5;28mself\u001b[39m.export_pb(keras_model=keras_model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lin40269\\Desktop\\Linh\\01_Python\\realsense\\.venv\\Lib\\site-packages\\ultralytics\\engine\\exporter.py:198\u001b[39m, in \u001b[36mtry_export.<locals>.outer_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    197\u001b[39m     LOGGER.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m export failure \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt.t\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lin40269\\Desktop\\Linh\\01_Python\\realsense\\.venv\\Lib\\site-packages\\ultralytics\\engine\\exporter.py:193\u001b[39m, in \u001b[36mtry_export.<locals>.outer_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Profile() \u001b[38;5;28;01mas\u001b[39;00m dt:\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m         f, model = \u001b[43minner_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m     LOGGER.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m export success ✅ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt.t\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms, saved as \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_size(f)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m MB)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f, model\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lin40269\\Desktop\\Linh\\01_Python\\realsense\\.venv\\Lib\\site-packages\\ultralytics\\engine\\exporter.py:996\u001b[39m, in \u001b[36mExporter.export_saved_model\u001b[39m\u001b[34m(self, prefix)\u001b[39m\n\u001b[32m    993\u001b[39m         np.save(\u001b[38;5;28mstr\u001b[39m(tmp_file), images.numpy().astype(np.float32))  \u001b[38;5;66;03m# BHWC\u001b[39;00m\n\u001b[32m    994\u001b[39m         np_data = [[\u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m, tmp_file, [[[[\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m]]]], [[[[\u001b[32m255\u001b[39m, \u001b[32m255\u001b[39m, \u001b[32m255\u001b[39m]]]]]]\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01monnx2tf\u001b[39;00m  \u001b[38;5;66;03m# scoped for after ONNX export for reduced conflict during import\u001b[39;00m\n\u001b[32m    998\u001b[39m LOGGER.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m starting TFLite export with onnx2tf \u001b[39m\u001b[38;5;132;01m{\u001b[39;00monnx2tf.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    999\u001b[39m keras_model = onnx2tf.convert(\n\u001b[32m   1000\u001b[39m     input_onnx_file_path=f_onnx,\n\u001b[32m   1001\u001b[39m     output_folder_path=\u001b[38;5;28mstr\u001b[39m(f),\n\u001b[32m   (...)\u001b[39m\u001b[32m   1010\u001b[39m     optimization_for_gpu_delegate=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1011\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lin40269\\Desktop\\Linh\\01_Python\\realsense\\.venv\\Lib\\site-packages\\onnx2tf\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01monnx2tf\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01monnx2tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert, main\n\u001b[32m      3\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m1.28.1\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lin40269\\Desktop\\Linh\\01_Python\\realsense\\.venv\\Lib\\site-packages\\onnx2tf\\onnx2tf.py:44\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01margparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArgumentParser\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01monnx2tf\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     45\u001b[39m     dummy_onnx_inference,\n\u001b[32m     46\u001b[39m     dummy_tf_inference,\n\u001b[32m     47\u001b[39m     onnx_tf_tensor_validation,\n\u001b[32m     48\u001b[39m     weights_export,\n\u001b[32m     49\u001b[39m     download_test_image_data,\n\u001b[32m     50\u001b[39m     get_tf_model_inputs,\n\u001b[32m     51\u001b[39m     get_tf_model_outputs,\n\u001b[32m     52\u001b[39m     rewrite_tflite_inout_opname,\n\u001b[32m     53\u001b[39m     check_cuda_enabled,\n\u001b[32m     54\u001b[39m )\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01monnx2tf\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjson_auto_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     56\u001b[39m     generate_auto_replacement_json,\n\u001b[32m     57\u001b[39m     save_auto_replacement_json,\n\u001b[32m     58\u001b[39m )\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01monnx2tf\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menums\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     60\u001b[39m     CUDA_ONLY_OPS,\n\u001b[32m     61\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lin40269\\Desktop\\Linh\\01_Python\\realsense\\.venv\\Lib\\site-packages\\onnx2tf\\utils\\common_functions.py:20\u001b[39m\n\u001b[32m     18\u001b[39m np.random.seed(\u001b[32m0\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mai_edge_litert\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minterpreter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interpreter\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_keras\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Lambda\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ai_edge_litert'"
     ]
    }
   ],
   "source": [
    "model = YOLO(MODEL_PATH)\n",
    "model.export(format='tflite', data=r\"Focus1\\Focus1\\new dataset_2000\\batrec.v9-allen_yolo8_11112024.yolov8-obb\\data.yaml\", int8 = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67c0541",
   "metadata": {},
   "source": [
    "# Quantize model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ea8ef",
   "metadata": {},
   "source": [
    "## Dynamic Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7024c5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input=r\"models\\focus1\\Focus1_YOLO11s_x1024_14112024.onnx\",\n",
    "    model_output=r\"models\\focus1\\Focus1_YOLO11s_quant.onnx\",\n",
    "    weight_type=QuantType.QInt8  # or QUInt8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beadfd11",
   "metadata": {},
   "source": [
    "## Static quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e511880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import CalibrationDataReader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "class ImageFolderCalibrationReader(CalibrationDataReader):\n",
    "    def __init__(self, image_dir, input_name, input_size=(640, 640), max_samples=10):\n",
    "        self.input_name = input_name\n",
    "        image_paths = glob.glob(f\"{image_dir}/*.jpg\")[:max_samples]\n",
    "        self.data = []\n",
    "\n",
    "        for path in image_paths:\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.resize(img, input_size)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = img.transpose(2, 0, 1) / 255.0\n",
    "            tensor = np.expand_dims(img, axis=0).astype(np.float32)\n",
    "            self.data.append({self.input_name: tensor})\n",
    "\n",
    "        self.iterator = iter(self.data)\n",
    "\n",
    "    def get_next(self):\n",
    "        return next(self.iterator, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c1a4d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_static, QuantFormat, QuantType\n",
    "\n",
    "# Replace with your actual input name (e.g., \"images\")\n",
    "input_name = \"images\"\n",
    "calibration_reader = ImageFolderCalibrationReader(\n",
    "    image_dir=r\"Focus1\\Focus1\\new dataset_2000\\batrec.v9-allen_yolo8_11112024.yolov8-obb\\test\\images\", \n",
    "    input_name=input_name,\n",
    "    input_size=(640, 640)\n",
    ")\n",
    "\n",
    "quantize_static(\n",
    "    model_input=r\"models\\focus1\\Focus1_YOLO11s_x1024_14112024.onnx\",\n",
    "    model_output=r\"models\\focus1\\Focus1_YOLO11s_quant_static.onnx\",\n",
    "    calibration_data_reader=calibration_reader,\n",
    "    quant_format=QuantFormat.QDQ,  # Use QDQ format\n",
    "    weight_type=QuantType.QInt8,\n",
    "    activation_type=QuantType.QInt8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17efd4ba",
   "metadata": {},
   "source": [
    "# Test run model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e10628c",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5464b96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# Load model\n",
    "session = ort.InferenceSession(\"yolov11.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "# Get input name\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e576b111",
   "metadata": {},
   "source": [
    "## Video inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63a7dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "def preprocess(frame, input_size=640):\n",
    "    img = cv2.resize(frame, (input_size, input_size))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.transpose(2, 0, 1) / 255.0\n",
    "    return img[np.newaxis, ...].astype(np.float32)\n",
    "\n",
    "def nms(boxes, scores, iou_threshold=0.5):\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, scores, score_threshold=0.25, nms_threshold=iou_threshold)\n",
    "    \n",
    "    if indices is None or len(indices) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Flatten if needed\n",
    "    if isinstance(indices, np.ndarray):\n",
    "        indices = indices.flatten().tolist()\n",
    "    elif isinstance(indices[0], (list, tuple)):\n",
    "        indices = [i[0] for i in indices]\n",
    "    \n",
    "    return indices\n",
    "\n",
    "def postprocess(preds, frame_shape, input_size=640, conf_thres=0.25, iou_thres=0.45):\n",
    "    detections = []\n",
    "    pred = preds[0][0]  # shape: (N, 85)\n",
    "\n",
    "    for det in pred:\n",
    "        conf = det[4]\n",
    "        if conf < conf_thres:\n",
    "            continue\n",
    "        class_id = np.argmax(det[5:])\n",
    "        score = conf * det[5:][class_id]\n",
    "        x, y, w, h = det[:4]\n",
    "        x1 = int((x - w / 2) / input_size * frame_shape[1])\n",
    "        y1 = int((y - h / 2) / input_size * frame_shape[0])\n",
    "        x2 = int((x + w / 2) / input_size * frame_shape[1])\n",
    "        y2 = int((y + h / 2) / input_size * frame_shape[0])\n",
    "        detections.append(([x1, y1, x2 - x1, y2 - y1], score, class_id))\n",
    "\n",
    "    if not detections:\n",
    "        return []\n",
    "    \n",
    "    boxes, scores, class_ids = zip(*detections)\n",
    "    keep = nms(boxes, scores, iou_threshold=iou_thres)\n",
    "    return [(*boxes[i], scores[i], class_ids[i]) for i in keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0f8d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_INFERENCE = r\"models\\focus1\\Focus1_YOLO11s_x1024_14112024.onnx\"\n",
    "\n",
    "session = ort.InferenceSession(MODEL_PATH_INFERENCE)\n",
    "input_name = session.get_inputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c244ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyrealsense2.pyrealsense2.pipeline_profile at 0x22509e82030>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Setup RealSense pipeline\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "pipeline.start(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "185a9677",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        if not color_frame:\n",
    "            continue\n",
    "\n",
    "        frame = np.asanyarray(color_frame.get_data())\n",
    "        input_tensor = preprocess(frame)\n",
    "        outputs = session.run(None, {input_name: input_tensor})\n",
    "        detections = postprocess(outputs, frame.shape[:2])\n",
    "\n",
    "        for x1, y1, x2, y2, score, cls in detections:\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{cls}:{score:.2f}\", (x1, y1 - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "\n",
    "        cv2.imshow(\"YOLOv11 + RealSense\", frame)\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    pipeline.stop()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
